{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NFDI MatWerk Ontology (MWO) NFDI-MatWerk aims to establish a digital infrastructure for Materials Science and Engineering (MSE), fostering improved data sharing and collaboration. This repository provides comprehensive documentation for NFDI MatWerk Ontology (MWO) v3.0, a foundational framework designed to structure research data and enhance interoperability within the MSE community. To ensure compliance with top-level ontology standards, MWO v3.0 is aligned with the Basic Formal Ontology (BFO) and incorporates the modular approach of the NFDIcore mid-level ontology, enriching metadata through standardized classes and properties. The MWO addresses key aspects of MSE research data, including the NFDI-MatWerk community structure, covering task areas, infrastructure use cases, projects, researchers, and organizations. It also describes essential NFDI resources, such as software, workflows, ontologies, publications, datasets, metadata schemas, instruments, facilities, and educational materials. Additionally, MWO represents NFDI-MatWerk services, academic events, courses, and international collaborations. As the foundation for the MSE Knowledge Graph, MWO facilitates efficient data integration and retrieval, promoting collaboration and knowledge representation across MSE domains. This digital transformation enhances data discoverability, reusability, and accelerates scientific exchange, innovation, and discoveries by optimizing research data management and accessibility. The ontology\u2019s latest version is always accessible at: mwo.owl . Ontology metadata Title: NFDI MatWerk Ontology. Abbreviation: mwo Namespace: http://purls.helmholtz-metadaten.de/mwo . Prefix: mwo Language: OWL Repository: https://github.com/ISE-FIZKarlsruhe/mwo Previous version (MWO v2, 2024.03.11): http://purls.helmholtz-metadaten.de/mwo/2.0.0 . Previous version (MWO v1, 2023.02.16): http://purls.helmholtz-metadaten.de/mwo/1.0.1 . Creators: Hossein Beygi Nasrabadi, J\u00f6rg Waitelonis, Ebrahim Norouzi, Kostiantyn Hubaiev, Harald Sack. Contributors: Abril Az\u00f3car Guzm\u00e1n, Ahmad Zainul Ihsan, Said Fathalla, Angelika Gedsun, Amir Laadhar, Stefan Sandfeld, Volker Hofmann, Felix Fritzen. Related project: NFDI MatWerk. Funding: Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the National Research Data Infrastructure \u2013 NFDI 38/1 \u2013 project number 460247524. License: https://creativecommons.org/publicdomain/zero/1.0/ . Citation: Hossein Beygi Nasrabadi, J\u00f6rg Waitelonis, Ebrahim Norouzi, Kostiantyn Hubaiev, Harald Sack. NFDI MatWerk Ontology (mwo). Revision: v3.0.0. Retrieved from: https://github.com/ISE-FIZKarlsruhe/mwo . Here you will find information about : - Introduction, scopes, features, and application of the ontology - Ontology structure, reused upper-level ontologies, and ontology key concepts - Ontology patterns and use-cases - Different versions of the ontology - how to contribute - Acknowledgements - References and related publications","title":"Welcome"},{"location":"#nfdi-matwerk-ontology-mwo","text":"NFDI-MatWerk aims to establish a digital infrastructure for Materials Science and Engineering (MSE), fostering improved data sharing and collaboration. This repository provides comprehensive documentation for NFDI MatWerk Ontology (MWO) v3.0, a foundational framework designed to structure research data and enhance interoperability within the MSE community. To ensure compliance with top-level ontology standards, MWO v3.0 is aligned with the Basic Formal Ontology (BFO) and incorporates the modular approach of the NFDIcore mid-level ontology, enriching metadata through standardized classes and properties. The MWO addresses key aspects of MSE research data, including the NFDI-MatWerk community structure, covering task areas, infrastructure use cases, projects, researchers, and organizations. It also describes essential NFDI resources, such as software, workflows, ontologies, publications, datasets, metadata schemas, instruments, facilities, and educational materials. Additionally, MWO represents NFDI-MatWerk services, academic events, courses, and international collaborations. As the foundation for the MSE Knowledge Graph, MWO facilitates efficient data integration and retrieval, promoting collaboration and knowledge representation across MSE domains. This digital transformation enhances data discoverability, reusability, and accelerates scientific exchange, innovation, and discoveries by optimizing research data management and accessibility. The ontology\u2019s latest version is always accessible at: mwo.owl .","title":"NFDI MatWerk Ontology (MWO)"},{"location":"#ontology-metadata","text":"Title: NFDI MatWerk Ontology. Abbreviation: mwo Namespace: http://purls.helmholtz-metadaten.de/mwo . Prefix: mwo Language: OWL Repository: https://github.com/ISE-FIZKarlsruhe/mwo Previous version (MWO v2, 2024.03.11): http://purls.helmholtz-metadaten.de/mwo/2.0.0 . Previous version (MWO v1, 2023.02.16): http://purls.helmholtz-metadaten.de/mwo/1.0.1 . Creators: Hossein Beygi Nasrabadi, J\u00f6rg Waitelonis, Ebrahim Norouzi, Kostiantyn Hubaiev, Harald Sack. Contributors: Abril Az\u00f3car Guzm\u00e1n, Ahmad Zainul Ihsan, Said Fathalla, Angelika Gedsun, Amir Laadhar, Stefan Sandfeld, Volker Hofmann, Felix Fritzen. Related project: NFDI MatWerk. Funding: Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the National Research Data Infrastructure \u2013 NFDI 38/1 \u2013 project number 460247524. License: https://creativecommons.org/publicdomain/zero/1.0/ . Citation: Hossein Beygi Nasrabadi, J\u00f6rg Waitelonis, Ebrahim Norouzi, Kostiantyn Hubaiev, Harald Sack. NFDI MatWerk Ontology (mwo). Revision: v3.0.0. Retrieved from: https://github.com/ISE-FIZKarlsruhe/mwo . Here you will find information about : - Introduction, scopes, features, and application of the ontology - Ontology structure, reused upper-level ontologies, and ontology key concepts - Ontology patterns and use-cases - Different versions of the ontology - how to contribute - Acknowledgements - References and related publications","title":"Ontology metadata"},{"location":"acknowledgements/","text":"Acknowledgements The NFDI MatWek project funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the National Research Data Infrastructure \u2013 NFDI 38/1 \u2013 project number 460247524.","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"The NFDI MatWek project funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under the National Research Data Infrastructure \u2013 NFDI 38/1 \u2013 project number 460247524.","title":"Acknowledgements"},{"location":"cite/","text":"How to cite MWO","title":"How to cite MWO"},{"location":"cite/#how-to-cite-mwo","text":"","title":"How to cite MWO"},{"location":"contributing/","text":"Contributing If you are a researcher, developer, or industry professional interested in contributing to the development, support, or reuse of the NFDI MatWerk Ontology (MWO), we invite you to explore and engage with our project repository. Your participation will help enhance the ontology\u2019s capabilities and ensure its continuous improvement for the MSE community. We welcome contributions to the NFDI MatWerk Ontology (MWO)! To get involved: Visit our GitHub repository : Explore the latest developments and ongoing discussions. Create an issue : Report bugs, suggest improvements, or request new features. Your contributions will help improve MWO for the MSE community. Join us on GitHub and be part of the development!","title":"Contributing"},{"location":"contributing/#contributing","text":"If you are a researcher, developer, or industry professional interested in contributing to the development, support, or reuse of the NFDI MatWerk Ontology (MWO), we invite you to explore and engage with our project repository. Your participation will help enhance the ontology\u2019s capabilities and ensure its continuous improvement for the MSE community. We welcome contributions to the NFDI MatWerk Ontology (MWO)! To get involved: Visit our GitHub repository : Explore the latest developments and ongoing discussions. Create an issue : Report bugs, suggest improvements, or request new features. Your contributions will help improve MWO for the MSE community. Join us on GitHub and be part of the development!","title":"Contributing"},{"location":"intro/","text":"Introduction The National Research Data Infrastructure (NFDI) is a German initiative established in 2020 to systematically manage scientific and research data. It aims to provide long-term data storage, backup, and accessibility while enabling the networking of research data both nationally and internationally. By developing a sustainable and standardized research data infrastructure across various disciplines, NFDI enhances data management practices and ensures the long-term availability of valuable research outputs [1, 2]. As a specialized consortium within the NFDI framework, NFDI-MatWerk (National Research Data Infrastructure for Materials Science and Engineering) aims to develop a digital infrastructure for Materials Science and Engineering (MSE). Funded by the German Research Foundation (DFG), it focuses on the integration and standardization of decentralized data, metadata, workflows, and ontologies to enhance interoperability and reproducibility in MSE research. By incorporating both experimental and simulation data, along with their associated metadata, NFDI-MatWerk provides essential tools and frameworks for efficient data management, long-term accessibility, and improved collaboration within the field [3]. The NFDI-MatWerk Ontology (MWO) is a domain-level ontology that designed to standardize terminology, enhance data integration, and improve collaboration in MSE. One of the key objectives MWO is to develop a knowledge graph that integrates FAIR digital objects, enabling the structured representation of materials and processing parameters. By ensuring the seamless integration of decentralized data and workflows, MWO plays a crucial role in enhancing interoperability and reproducibility across MSE research. The NFDI MatWerk Ontology (MWO) is a unified framework developed by the Task Area Ontologies for Materials Science (TA-OMS) within the NFDI-MatWerk initiative [4]. Goals The ontology development task area within NFDI-MatWerk aims to establish a structured and standardized approach to managing MSE data. By creating a comprehensive and interoperable ontology, this effort seeks to enhance data integration, discovery, and reuse while ensuring alignment with community standards and evolving research needs. The key goals of this initiative include [4]: - Develop Unified Materials Ontology : Create a comprehensive ontology that standardizes the representation of materials data, including individualized ontologies for specific tools and workflows developed within the community. - Ensure Interoperability : Facilitate the integration and interoperability of materials data by providing a common framework for data descriptions and metadata alignment. - Support Efficient Data Discovery : Enhance the ability of researchers to discover and access relevant materials data through standardized and easily searchable metadata. - Promote Data Integration and Reuse : Enable efficient integration and reuse of data across various platforms and research activities by implementing consistent ontological frameworks. - Implement a Knowledge Graph : Develop and maintain a knowledge graph that allows for the retrieval of data based on their metadata, improving data accessibility and usability. - Align with Community Standards : Ensure that the ontology aligns with existing community standards and practices, facilitating broad acceptance and adoption within the materials science community. - Adapt to Emerging Needs and Technologies : Continuously update and refine the ontology to address emerging research needs and incorporate new technologies and methodologies in materials science. - Enhance Metadata Quality : Improve the quality of metadata by providing guidelines and tools for creating detailed and accurate data descriptions. Ontology scope As a domain ontology, MWO serves as a foundational framework for the MSE community, structuring research data and enhancing accessibility and interoperability. The key aspects of this ontology\u2019s scope include: A) Consortium structure: NFDI MatWerk consortium, its structures (task areas, infrastructure use cases, and participant projects), researches, and organizations, B) NFDI resources: software, data portals, metadata schemas, scientific publications, published datasets, workflows, and ontologies, C) NFDI MatWerk services, events, educational coerces, and international collaborations, D) MSE related instruments, facilities, materials, processes, and properties. MWO v3.0 key features. Alignment with standards : Conforming ISO/IEC 21838-2:2021 standard [5], MWO v3.0 builds upon the Basic Formal Ontology (BFO) top level ontology. Reusability and extensibility : MWO v3.0 builds upon the NFDIcore v3 mid-level ontology and integrates with Platform MaterialDigital ontology (PMDco v3.0). Furthermore, MWO v3.0 incorporates entities from various other ontologies like IAO, SWO, DCAT, and Schema, ensuring broad applicability and cross-domain connectivity. Improved semantic richness : Enhancing the expressiveness and depth of ontology terminology to better capture the requirements of the community and the materials science and engineering knowledge graph. Adaptive and community-driven development : The ontology is continuously extended and maintained through community-driven development, ensuring it adapts to the evolving needs of MSE research data. Standardized Ontology Development and Maintenance : the MWO v3.0 is released and documented using the OBO-based Ontology Development Kit (ODK) [6], which ensures a standardized development workflow aligned with OBO Foundry best practices. ODK provides automated quality control, enhancing ontology consistency and reducing errors. It enables seamless integration with other OBO ontologies, supports FAIR principles, and streamlines versioning and release management. Additionally, it facilitates collaborative development through GitHub-based workflows, ensuring continuous improvement and broad adoption within the MSE research community. Applications At the moment, MWO is being used as a basis for developing several application ontologies for the MSE research data management. These ontologies will be listed here as soon as their release. Furthermore, MWO is the basis for the MSE knowledge graph (MSE-KG) [7]. It is a structured digital resource designed to integrate and provide unified access to distributed and heterogeneous research data within the NFDI-MatWerk consortium and the broader MSE community. Serving as the backend data resource for the NFDI-MatWerk portal, it enables researchers to explore community structures, infrastructure, and scientific data through a standardized and continuously updated platform. Built on W3C technologies and deployed using Wikibase, the MSE-KG facilitates semantic data integration, enhances discoverability, and supports ontology-based knowledge representation in materials science and engineering.","title":"Introduction"},{"location":"intro/#introduction","text":"The National Research Data Infrastructure (NFDI) is a German initiative established in 2020 to systematically manage scientific and research data. It aims to provide long-term data storage, backup, and accessibility while enabling the networking of research data both nationally and internationally. By developing a sustainable and standardized research data infrastructure across various disciplines, NFDI enhances data management practices and ensures the long-term availability of valuable research outputs [1, 2]. As a specialized consortium within the NFDI framework, NFDI-MatWerk (National Research Data Infrastructure for Materials Science and Engineering) aims to develop a digital infrastructure for Materials Science and Engineering (MSE). Funded by the German Research Foundation (DFG), it focuses on the integration and standardization of decentralized data, metadata, workflows, and ontologies to enhance interoperability and reproducibility in MSE research. By incorporating both experimental and simulation data, along with their associated metadata, NFDI-MatWerk provides essential tools and frameworks for efficient data management, long-term accessibility, and improved collaboration within the field [3]. The NFDI-MatWerk Ontology (MWO) is a domain-level ontology that designed to standardize terminology, enhance data integration, and improve collaboration in MSE. One of the key objectives MWO is to develop a knowledge graph that integrates FAIR digital objects, enabling the structured representation of materials and processing parameters. By ensuring the seamless integration of decentralized data and workflows, MWO plays a crucial role in enhancing interoperability and reproducibility across MSE research. The NFDI MatWerk Ontology (MWO) is a unified framework developed by the Task Area Ontologies for Materials Science (TA-OMS) within the NFDI-MatWerk initiative [4].","title":"Introduction"},{"location":"intro/#goals","text":"The ontology development task area within NFDI-MatWerk aims to establish a structured and standardized approach to managing MSE data. By creating a comprehensive and interoperable ontology, this effort seeks to enhance data integration, discovery, and reuse while ensuring alignment with community standards and evolving research needs. The key goals of this initiative include [4]: - Develop Unified Materials Ontology : Create a comprehensive ontology that standardizes the representation of materials data, including individualized ontologies for specific tools and workflows developed within the community. - Ensure Interoperability : Facilitate the integration and interoperability of materials data by providing a common framework for data descriptions and metadata alignment. - Support Efficient Data Discovery : Enhance the ability of researchers to discover and access relevant materials data through standardized and easily searchable metadata. - Promote Data Integration and Reuse : Enable efficient integration and reuse of data across various platforms and research activities by implementing consistent ontological frameworks. - Implement a Knowledge Graph : Develop and maintain a knowledge graph that allows for the retrieval of data based on their metadata, improving data accessibility and usability. - Align with Community Standards : Ensure that the ontology aligns with existing community standards and practices, facilitating broad acceptance and adoption within the materials science community. - Adapt to Emerging Needs and Technologies : Continuously update and refine the ontology to address emerging research needs and incorporate new technologies and methodologies in materials science. - Enhance Metadata Quality : Improve the quality of metadata by providing guidelines and tools for creating detailed and accurate data descriptions.","title":"Goals"},{"location":"intro/#ontology-scope","text":"As a domain ontology, MWO serves as a foundational framework for the MSE community, structuring research data and enhancing accessibility and interoperability. The key aspects of this ontology\u2019s scope include: A) Consortium structure: NFDI MatWerk consortium, its structures (task areas, infrastructure use cases, and participant projects), researches, and organizations, B) NFDI resources: software, data portals, metadata schemas, scientific publications, published datasets, workflows, and ontologies, C) NFDI MatWerk services, events, educational coerces, and international collaborations, D) MSE related instruments, facilities, materials, processes, and properties.","title":"Ontology scope"},{"location":"intro/#mwo-v30-key-features","text":"Alignment with standards : Conforming ISO/IEC 21838-2:2021 standard [5], MWO v3.0 builds upon the Basic Formal Ontology (BFO) top level ontology. Reusability and extensibility : MWO v3.0 builds upon the NFDIcore v3 mid-level ontology and integrates with Platform MaterialDigital ontology (PMDco v3.0). Furthermore, MWO v3.0 incorporates entities from various other ontologies like IAO, SWO, DCAT, and Schema, ensuring broad applicability and cross-domain connectivity. Improved semantic richness : Enhancing the expressiveness and depth of ontology terminology to better capture the requirements of the community and the materials science and engineering knowledge graph. Adaptive and community-driven development : The ontology is continuously extended and maintained through community-driven development, ensuring it adapts to the evolving needs of MSE research data. Standardized Ontology Development and Maintenance : the MWO v3.0 is released and documented using the OBO-based Ontology Development Kit (ODK) [6], which ensures a standardized development workflow aligned with OBO Foundry best practices. ODK provides automated quality control, enhancing ontology consistency and reducing errors. It enables seamless integration with other OBO ontologies, supports FAIR principles, and streamlines versioning and release management. Additionally, it facilitates collaborative development through GitHub-based workflows, ensuring continuous improvement and broad adoption within the MSE research community.","title":"MWO v3.0 key features."},{"location":"intro/#applications","text":"At the moment, MWO is being used as a basis for developing several application ontologies for the MSE research data management. These ontologies will be listed here as soon as their release. Furthermore, MWO is the basis for the MSE knowledge graph (MSE-KG) [7]. It is a structured digital resource designed to integrate and provide unified access to distributed and heterogeneous research data within the NFDI-MatWerk consortium and the broader MSE community. Serving as the backend data resource for the NFDI-MatWerk portal, it enables researchers to explore community structures, infrastructure, and scientific data through a standardized and continuously updated platform. Built on W3C technologies and deployed using Wikibase, the MSE-KG facilitates semantic data integration, enhances discoverability, and supports ontology-based knowledge representation in materials science and engineering.","title":"Applications"},{"location":"ontology/","text":"Ontology structure MWO v3.0 is a domain-level ontology designed to enhance data integration and retrieval, fostering collaboration and improving knowledge representation across MSE domains. It covers essential aspects of MSE research data, including the NFDI-MatWerk community structure, key NFDI resources, services, and MSE-related research data description. By enabling the integration of diverse data sources and supporting complex research data workflows, MWO bridges MSE research metadata with concepts from upper-level ontologies. Reused upper-level ontologies Building the MWO based on standardized upper-level ontologies provides a well-designed and standardized semantic structure, ensuring clear definitions of entities and their relationships. These ontologies enhance interoperability across different domains, support ontology development, and promote consistency in knowledge representation. The following upper-level ontologies have been incorporated for MWO development: - Basic Formal Ontology (BFO 2020) : BFO was selected as the top-level ontology due to its well-structured design, broad applicability, and ability to integrate with various ontologies [8]. As a foundational framework, BFO provides abstract, cross-domain semantic structures, ensuring comprehensive integration and compliance with top-level ontology standards. Its adherence to the ISO/IEC 21838-2 standard [5] further enhances interoperability in developed ontologies, facilitating knowledge representation, data exchange, and interdisciplinary collaboration. - NFDIcore ontology (v3.0) : As a mid-level ontology, NFDIcore plays a central role in structuring and integrating research data across consortia. NFDIcore features a modular structure for improved interoperability among NFDI consortia. It provides a shared vocabulary that represents both the organizational structure of the NFDI and the diverse datasets contributed by project partners. The ontology encompasses key concepts such as organizations, consortia, projects, datasets, research outputs, geographical locations, and technical standards. These structured representations enable efficient data management, integration, and reuse across disciplines [9]. - Platform MaterialDigital Core Ontology (PMDco v3.0) : The PMDco is a mid-level ontology in the MSE domain, and provides bridging mid-level concepts for detailed description of processes, experiments, and computational workflows enabling the reproducibility of process and materials data. These general MSE concepts are designed to be extendable for specific applications within application ontologies. The PMDco is designed in a collaborative effort within the MaterialDigital initiative and intended to be easily used by MSE domain experts [10]. - Artifact Ontology (IAO) : Since BFO 2020 (the version currently adopted by NFDIcore) is not fully supported by IAO [11], some relevant IAO concepts could not be reused. In such cases, NFDIcore-specific classes have been introduced, such as dataset, document and identifier. - Other reused ontologies and vocabularies like Software Ontology (SWO) [12], Data Catalog Vocabulary (DCAT) [13], and schema.org vocabulary [14]. Key concepts Classes Below figure illustrates the development of MWO concepts by reusing upper-level ontologies. The top-level (red-colored entities at the upper side) is based on BFO, while the mid-level consists of NFDI Core (blue), PMD (yellow), and IAO (purple). The domain-level, represented in green, corresponds to MWO-specific concepts. Notably, most MWO extensions appear under the bfo:GenericallyDependentContinuant class, indicating a strong focus on information-related research data descriptions. This hierarchical structure ensures alignment with foundational ontological principles while accommodating domain-specific needs. Regarding the scope of the ontology, the key concepts in MWO v3.0 are categorized into four main groups. Below is an overview of these core concepts. The full list of concepts is available in the generated ontology description: https://nfdi.fiz-karlsruhe.de/mwo/ . A) Consortium Structure: Covers the NFDI MatWerk consortium, including MatWerk TA, IUC, PP organizations, as well as the specifications of involved people and organizations. - NFDI MatWerk Consortium: NFDI-MatWerk will focus on the research areas of materials science and materials engineering. The key challenges in these fields are the digital mapping of materials and their process and loading parameters. This process touches core aspects of scientific ways of working starting with scientific exchange, data handling and the resulting technological possibilities. The digital transformation of materials science and materials engineering is an opportunity to promote, structure and optimize this exchange - provided that transparent communication standards are created. This fundamental change is therefore being tackled in a joint effort by the consortium and the specialist community. - MatWerk Project: A MatWerk project is a project within the NFDI MatWerk consortium that involves coordinated research, development, or infrastructure activities aimed at advancing materials science and engineering through data-driven methodologies and digitalization. - MatWerk Participating project: Participant Projects (PPs) are projects by NFDI-MatWerk partners that address specific scientific and technological challenges, providing practical examples and feedback for the development and validation of the infrastructure. - Persons, organizations and their roles: a wide varieties of roles in project and related person and organization entities were created in subcleaase of nfdicore:Agent and nfdicore:AgentRole. B) NFDI resources: including implemented tools, software, publications, datasets, data portals, and metadata specifications. - Materials science subject areas classified in 10 groups of computational materials science, data driven material science, materials characterization, materials deformation processes, materials design, materials mechanical behavior, materials microstructural, materials processing, materials surface science, and materials thermodynamics. - Data portals subclasses of data analysis portal, linked data portal, material science database, material science knowledge base, repository hosting data portal, research data repository, and software repository. - Software subclasses like data analysis software, electronic lab notebook, image processing software, operating system, simulation software, and workflow management software. - Documents (e.g., presentation document) and publication types like conference abstract, conference paper, conference poster, and, journal article. - Document parts like authors/creators/contributors list and institutions list. - Metadata specification, a detailed document that describes metadata structures, guidelines, and usage policies for standardization and adoption. - Datasets (experimental, simulation and reference dataset). A Reference Dataset is a dataset that serves as a benchmark or standardized collection of data used for validation, comparison, and reproducibility in scientific research and engineering applications. - Information access specification, a directive information entity that defines the policies, conditions, and mechanisms governing the accessibility of digital or physical information resources for users or organizations. C) Service, events, educational coerces, and collaborations: - Collaboration: A collaboration is a process in which multiple entities, such as individuals, organizations, or institutions, work together toward shared goals, contributing resources, expertise, or services to achieve mutual benefits. - Infrastructure Use Case (IUC): a service product that defines a specific application scenario, requirement, or implementation of research infrastructure, demonstrating its functionality, benefits, and impact in a given scientific or industrial domain. - Event implementation specification: a directive information entity that provides formal guidelines, constraints, and details on how an event should be executed. It specifies the mode of implementation, including logistical, technical, and procedural aspects required to carry out an event successfully. - Status specification: a directive information entity that defines the current state, progress, or condition of an entity, process, or project based on predefined criteria and operational guidelines. - Event frequency datum: describes how often an events occurs within a given time interval. - Sustainability datum: represents the duration or persistence of a process, system, or resource over time. It defines how long an entity is expected to remain viable or functional. D) Specification of MSE-related materials, instruments, and facilities: While most of the MSE-related concepts of MWO were derived from PMDco ontologies, some MSE-related entities were also represented in mwo to fascinate description of materials, instruments, and facilities. For example; - Large scale facility: A Large-Scale Facility is a service product that provides specialized, high-capacity infrastructure for scientific research, engineering, or computational purposes, supporting large-scale experiments, simulations, and data analysis in various domains. - Material designation: Material designation is an identifier assigned to a material to specify its name, Id, classification, composition, or standardized reference in scientific, engineering, or industrial contexts. Object properties Ontologies based on BFO are generally extended by adding classes, while extensive modifications to object properties are discouraged to maintain consistency and interoperability. Studies show that ontology developers using a foundational ontology tend to add more classes and fewer object properties than those who do not. However, in some cases, new object properties may be necessary. Based on MatWerk community request, we added just specific object properties of developed_by , codeveloped_by , and adapted_by to shortcut the relation between a NFDI resource and independent continuant (e.g., TA/IUC/PP organization). These properties support respond to competency questions like which tools are 'developed', 'co developed', or 'adopted' by NFDI MatWerk consortium. Data properties A limited number of data properties are used in MWO which more derived from NFDIcore ontology (e.g., has value, has url, and file extension). Below data properties were also defined by MWO: - has acronym: A relation between an information content entity and its specific acronym. - has id: A relation between an information content entity and its specific ID. Annotation properties No annotation property was created in MWO, as all required annotation properties are well developed by dc, dcterms, rdfs, owl, skos, etc. Individuals NFDI MatWerk Consortium instances: MatWerk task area (TA) organization, MatWerk participant project (PP) organization, and MatWerk infrastructure use case (IUC) organization, Status specifications: active, cancelled, finished, and paused statuses, Information access specifications: open, paywall or restricted accesses, Event implementation specifications: on-site, online and hybrid implementations.","title":"Ontology structure"},{"location":"ontology/#ontology-structure","text":"MWO v3.0 is a domain-level ontology designed to enhance data integration and retrieval, fostering collaboration and improving knowledge representation across MSE domains. It covers essential aspects of MSE research data, including the NFDI-MatWerk community structure, key NFDI resources, services, and MSE-related research data description. By enabling the integration of diverse data sources and supporting complex research data workflows, MWO bridges MSE research metadata with concepts from upper-level ontologies.","title":"Ontology structure"},{"location":"ontology/#reused-upper-level-ontologies","text":"Building the MWO based on standardized upper-level ontologies provides a well-designed and standardized semantic structure, ensuring clear definitions of entities and their relationships. These ontologies enhance interoperability across different domains, support ontology development, and promote consistency in knowledge representation. The following upper-level ontologies have been incorporated for MWO development: - Basic Formal Ontology (BFO 2020) : BFO was selected as the top-level ontology due to its well-structured design, broad applicability, and ability to integrate with various ontologies [8]. As a foundational framework, BFO provides abstract, cross-domain semantic structures, ensuring comprehensive integration and compliance with top-level ontology standards. Its adherence to the ISO/IEC 21838-2 standard [5] further enhances interoperability in developed ontologies, facilitating knowledge representation, data exchange, and interdisciplinary collaboration. - NFDIcore ontology (v3.0) : As a mid-level ontology, NFDIcore plays a central role in structuring and integrating research data across consortia. NFDIcore features a modular structure for improved interoperability among NFDI consortia. It provides a shared vocabulary that represents both the organizational structure of the NFDI and the diverse datasets contributed by project partners. The ontology encompasses key concepts such as organizations, consortia, projects, datasets, research outputs, geographical locations, and technical standards. These structured representations enable efficient data management, integration, and reuse across disciplines [9]. - Platform MaterialDigital Core Ontology (PMDco v3.0) : The PMDco is a mid-level ontology in the MSE domain, and provides bridging mid-level concepts for detailed description of processes, experiments, and computational workflows enabling the reproducibility of process and materials data. These general MSE concepts are designed to be extendable for specific applications within application ontologies. The PMDco is designed in a collaborative effort within the MaterialDigital initiative and intended to be easily used by MSE domain experts [10]. - Artifact Ontology (IAO) : Since BFO 2020 (the version currently adopted by NFDIcore) is not fully supported by IAO [11], some relevant IAO concepts could not be reused. In such cases, NFDIcore-specific classes have been introduced, such as dataset, document and identifier. - Other reused ontologies and vocabularies like Software Ontology (SWO) [12], Data Catalog Vocabulary (DCAT) [13], and schema.org vocabulary [14].","title":"Reused upper-level ontologies"},{"location":"ontology/#key-concepts","text":"","title":"Key concepts"},{"location":"ontology/#classes","text":"Below figure illustrates the development of MWO concepts by reusing upper-level ontologies. The top-level (red-colored entities at the upper side) is based on BFO, while the mid-level consists of NFDI Core (blue), PMD (yellow), and IAO (purple). The domain-level, represented in green, corresponds to MWO-specific concepts. Notably, most MWO extensions appear under the bfo:GenericallyDependentContinuant class, indicating a strong focus on information-related research data descriptions. This hierarchical structure ensures alignment with foundational ontological principles while accommodating domain-specific needs. Regarding the scope of the ontology, the key concepts in MWO v3.0 are categorized into four main groups. Below is an overview of these core concepts. The full list of concepts is available in the generated ontology description: https://nfdi.fiz-karlsruhe.de/mwo/ . A) Consortium Structure: Covers the NFDI MatWerk consortium, including MatWerk TA, IUC, PP organizations, as well as the specifications of involved people and organizations. - NFDI MatWerk Consortium: NFDI-MatWerk will focus on the research areas of materials science and materials engineering. The key challenges in these fields are the digital mapping of materials and their process and loading parameters. This process touches core aspects of scientific ways of working starting with scientific exchange, data handling and the resulting technological possibilities. The digital transformation of materials science and materials engineering is an opportunity to promote, structure and optimize this exchange - provided that transparent communication standards are created. This fundamental change is therefore being tackled in a joint effort by the consortium and the specialist community. - MatWerk Project: A MatWerk project is a project within the NFDI MatWerk consortium that involves coordinated research, development, or infrastructure activities aimed at advancing materials science and engineering through data-driven methodologies and digitalization. - MatWerk Participating project: Participant Projects (PPs) are projects by NFDI-MatWerk partners that address specific scientific and technological challenges, providing practical examples and feedback for the development and validation of the infrastructure. - Persons, organizations and their roles: a wide varieties of roles in project and related person and organization entities were created in subcleaase of nfdicore:Agent and nfdicore:AgentRole. B) NFDI resources: including implemented tools, software, publications, datasets, data portals, and metadata specifications. - Materials science subject areas classified in 10 groups of computational materials science, data driven material science, materials characterization, materials deformation processes, materials design, materials mechanical behavior, materials microstructural, materials processing, materials surface science, and materials thermodynamics. - Data portals subclasses of data analysis portal, linked data portal, material science database, material science knowledge base, repository hosting data portal, research data repository, and software repository. - Software subclasses like data analysis software, electronic lab notebook, image processing software, operating system, simulation software, and workflow management software. - Documents (e.g., presentation document) and publication types like conference abstract, conference paper, conference poster, and, journal article. - Document parts like authors/creators/contributors list and institutions list. - Metadata specification, a detailed document that describes metadata structures, guidelines, and usage policies for standardization and adoption. - Datasets (experimental, simulation and reference dataset). A Reference Dataset is a dataset that serves as a benchmark or standardized collection of data used for validation, comparison, and reproducibility in scientific research and engineering applications. - Information access specification, a directive information entity that defines the policies, conditions, and mechanisms governing the accessibility of digital or physical information resources for users or organizations. C) Service, events, educational coerces, and collaborations: - Collaboration: A collaboration is a process in which multiple entities, such as individuals, organizations, or institutions, work together toward shared goals, contributing resources, expertise, or services to achieve mutual benefits. - Infrastructure Use Case (IUC): a service product that defines a specific application scenario, requirement, or implementation of research infrastructure, demonstrating its functionality, benefits, and impact in a given scientific or industrial domain. - Event implementation specification: a directive information entity that provides formal guidelines, constraints, and details on how an event should be executed. It specifies the mode of implementation, including logistical, technical, and procedural aspects required to carry out an event successfully. - Status specification: a directive information entity that defines the current state, progress, or condition of an entity, process, or project based on predefined criteria and operational guidelines. - Event frequency datum: describes how often an events occurs within a given time interval. - Sustainability datum: represents the duration or persistence of a process, system, or resource over time. It defines how long an entity is expected to remain viable or functional. D) Specification of MSE-related materials, instruments, and facilities: While most of the MSE-related concepts of MWO were derived from PMDco ontologies, some MSE-related entities were also represented in mwo to fascinate description of materials, instruments, and facilities. For example; - Large scale facility: A Large-Scale Facility is a service product that provides specialized, high-capacity infrastructure for scientific research, engineering, or computational purposes, supporting large-scale experiments, simulations, and data analysis in various domains. - Material designation: Material designation is an identifier assigned to a material to specify its name, Id, classification, composition, or standardized reference in scientific, engineering, or industrial contexts.","title":"Classes"},{"location":"ontology/#object-properties","text":"Ontologies based on BFO are generally extended by adding classes, while extensive modifications to object properties are discouraged to maintain consistency and interoperability. Studies show that ontology developers using a foundational ontology tend to add more classes and fewer object properties than those who do not. However, in some cases, new object properties may be necessary. Based on MatWerk community request, we added just specific object properties of developed_by , codeveloped_by , and adapted_by to shortcut the relation between a NFDI resource and independent continuant (e.g., TA/IUC/PP organization). These properties support respond to competency questions like which tools are 'developed', 'co developed', or 'adopted' by NFDI MatWerk consortium.","title":"Object properties"},{"location":"ontology/#data-properties","text":"A limited number of data properties are used in MWO which more derived from NFDIcore ontology (e.g., has value, has url, and file extension). Below data properties were also defined by MWO: - has acronym: A relation between an information content entity and its specific acronym. - has id: A relation between an information content entity and its specific ID.","title":"Data properties"},{"location":"ontology/#annotation-properties","text":"No annotation property was created in MWO, as all required annotation properties are well developed by dc, dcterms, rdfs, owl, skos, etc.","title":"Annotation properties"},{"location":"ontology/#individuals","text":"NFDI MatWerk Consortium instances: MatWerk task area (TA) organization, MatWerk participant project (PP) organization, and MatWerk infrastructure use case (IUC) organization, Status specifications: active, cancelled, finished, and paused statuses, Information access specifications: open, paywall or restricted accesses, Event implementation specifications: on-site, online and hybrid implementations.","title":"Individuals"},{"location":"patterns/","text":"Patterns and use cases This section showcases example ontology design patterns (ODPs) developed using MWO and their application in representing practical MSE-based use cases, highlighting their capacity to enable scalable, reusable, and semantically robust knowledge frameworks. Ontology design patterns: ODPs are reusable and modular components that encapsulate best practices for solving common challenges in ontology development. Furthermore, ODPs may be used to create SHACL shapes to include constraints in a knowledge representation. By following usage patterns, ontology users and developers can ensure uniformity, clarity, and reusability in their models. The example structural, content-based, and presentation ODPs were designed to define foundational arrangements of entities, address domain-specific knowledge, and optimize user interaction with ontologies. Pattern 1: Process- Agent- Role The diagram illustrates the relationships between Occurrent, Independent Continuant, and Specifically Dependent Continuant in the context of BFO and MWO ontologies. Within NFDIcore, a process often serves to establish connections between agents and information content entities, defining the roles of agents with respect to the involved information content entities. Likewise, in NFDIcore, the concept of Agents extends to both organizations and persons, serving as independent continuants within the BFO ontology. Agents are essential for management of research data resources, and include entities such as research institutions, consortia, universities, companies, and individual researchers or data scientists. While bfo:Process represents an Occurrent (i.e., an event or process), nfdicore:Agent represents an Independent Continuant, which can act as a participant in a process, and bfo:Role represents a Specifically Dependent Continuant, which depends on an independent continuant. The object properties of has_participant, realizes, and bearer_of are all from BFO. This pattern shows how roles are assigned to agents participating in processes, maintaining alignment with BFO principles while extending them for MWO-specific applications. Pattern 2: Service The pattern represents a structured approach to modeling service processes within NFDIcore and MWO, following BFO principles. It illustrates how a service process involves an organization as a participant, which realizes a service provider role. The process generates an output (nfdicore:ServiceProduct), which realizes a service product role and is linked to a service specification (nfdicore:ServiceSpecification) through nfdicore:hasSpecification. This follows Pattern 1 for describing service processes, ensuring a structured linkage between processes, participants, roles, outputs, and specifications. Pattern 3: Resource- Contact point The pattern represents identifying the contact point of an NFDI resource. In NFDIcore, resources are continuants that encompass various digital creative works, such as datasets, collections, metadata, and offered products/services like data portals, data curation, and data digitization. Here, an NFDI resource is linked to a contact point agent through nfdicore:hasContactPoint. The agent, participating in a process, realizes a contact point role, which is further denoted by specific contact details, like a name, email address, and website. This structured approach ensures that each NFDI resource can be linked to a responsible contact agent with clearly defined roles and accessible communication channels, enhancing transparency and usability in NFDI-related services and digital infrastructures. Pattern 4: Resource implementation This diagram models the implementation relationship between an resource and its associated agent. The MatWerk project is represented as a process that involves a participant (like MatWerk TA, IUC, or PP) realizing a role. The output of this project is an NFDI resource (e.g., dataset, tool, software, or digital service). The property mwo:implementedBy provides a shortcut for linking the NFDI resource to the process that produced it, simplifying the tracking of tools, services, and digital outputs within the NFDI MatWerk consortium. This property was specifically introduced to address competency questions such as which tools were developed, co-developed, or adopted in an organization, enabling a more structured representation of research outputs and their contributors. Pattern 5: Resource description The pattern presents a detailed model for describing an NFDI resource (such as software, an ontology, a document, a publication, or a service) by incorporating multiple Information Content Entities (ICEs). Each NfdiResource is linked to various attributes through object properties, which are primarily refinements of iao:is_about. These attributes can include a wide range of ICEs like title, abbreviation, subject area, file format, specification, version, license, citation, standard, repository, external identifiers, and various textual entities (e.g., author information, keywords, requirements, and affiliations). Furthermore, NFDIcore defines specific sub-properties of iao:is_about, such as hasDataset, hasFileFormat, and hasLicence, to create a structured and semantically rich representation of digital resources. Pattern 6: Value specification This pattern illustrates how data values associated with various ICEs are represented using the nfdicore:hasValue data property. In general, all data values related to the above-mentioned patterns can be linked to ICEs via nfdicore:hasValue, ensuring a structured approach to representing numerical or textual values. More specifically, for measurement data, the measured value is assigned using nfdicore:hasValue, while its corresponding measurement unit is linked to a unit instance of iao:MeasurementUnitLabel via mwo:hasMeasurementUnitLabel. Pattern 7: Temporal region This pattern illustrates the representation of temporal regions within the BFO framework, distinguishing between zero-dimensional and one-dimensional temporal regions. A bfo:ZeroDimensionalTemporalRegion represents an instant in time, such as the start or end of an event, while a bfo:OneDimensionalTemporalRegion represents a time interval that has a duration and within which events can occur. The relationships between temporal entities, such as occupies_temporal_region, has_first_instance, and has_last_instance, are derived from BFO. To represent data associated with these temporal regions, mwo:SpecificationDatum, a subclass of iao:DataItem, is used. These data points, such as start date, end date, duration, and frequency, are linked to their values using the nfdicore:hasValue data property. Pattern 8: Material entity description The diagram provides a structured representation for Material Entities (such as devices or materials) descriptions. A Material Entity is connected to its relating processes or projects via bfo:participates_in or nfdicore:isOutputOf, indicating its involvement or production within a project. The realized aspects of a Material Entity, such as its role, disposition, and function, are represented through bfo:bearer_of and bfo:material_basis_of objerct properties. Additionally, agents (organizations, persons, service providers, or contact points) linked to the material entity are included via nfdicore:hasAgent. The location of the material entity is captured using nfdicore:Place, while its structural composition is detailed with obo:has_continuant_part or obo:has_member_part. Furthermore, various ICEs are used to describe the material entity\u2019s metadata, including identifiers, specifications, and other descriptive attributes, with nfdicore:hasSpecification linking the entity to its specifications. Use cases To illustrate the practical utility of ODPs, we presented several use cases within the MSE domain, such as representing metadata for materials, instruments, large-scale facilities, researchers, organizations, educational events, resources, datasets, data portals, and material research software. Through these cases, we emphasize how ODPs facilitate development by offering standardized templates for tasks such as hierarchy modeling, semantic relationship management, and data source integration, while also supporting ontology-driven solutions that are technically sound and adaptable to changing domain requirements. Use case 1- NFDI MatWerk organization Use case 2- NFDI MatWerk persons Use case 3- NFDI MatWerk task area (TA) Use case 4- Software description Use case 5- Event description Use case 5- large scale facility description","title":"Patterns and use cases"},{"location":"patterns/#patterns-and-use-cases","text":"This section showcases example ontology design patterns (ODPs) developed using MWO and their application in representing practical MSE-based use cases, highlighting their capacity to enable scalable, reusable, and semantically robust knowledge frameworks.","title":"Patterns and use cases"},{"location":"patterns/#ontology-design-patterns","text":"ODPs are reusable and modular components that encapsulate best practices for solving common challenges in ontology development. Furthermore, ODPs may be used to create SHACL shapes to include constraints in a knowledge representation. By following usage patterns, ontology users and developers can ensure uniformity, clarity, and reusability in their models. The example structural, content-based, and presentation ODPs were designed to define foundational arrangements of entities, address domain-specific knowledge, and optimize user interaction with ontologies.","title":"Ontology design patterns:"},{"location":"patterns/#pattern-1-process-agent-role","text":"The diagram illustrates the relationships between Occurrent, Independent Continuant, and Specifically Dependent Continuant in the context of BFO and MWO ontologies. Within NFDIcore, a process often serves to establish connections between agents and information content entities, defining the roles of agents with respect to the involved information content entities. Likewise, in NFDIcore, the concept of Agents extends to both organizations and persons, serving as independent continuants within the BFO ontology. Agents are essential for management of research data resources, and include entities such as research institutions, consortia, universities, companies, and individual researchers or data scientists. While bfo:Process represents an Occurrent (i.e., an event or process), nfdicore:Agent represents an Independent Continuant, which can act as a participant in a process, and bfo:Role represents a Specifically Dependent Continuant, which depends on an independent continuant. The object properties of has_participant, realizes, and bearer_of are all from BFO. This pattern shows how roles are assigned to agents participating in processes, maintaining alignment with BFO principles while extending them for MWO-specific applications.","title":"Pattern 1: Process- Agent- Role"},{"location":"patterns/#pattern-2-service","text":"The pattern represents a structured approach to modeling service processes within NFDIcore and MWO, following BFO principles. It illustrates how a service process involves an organization as a participant, which realizes a service provider role. The process generates an output (nfdicore:ServiceProduct), which realizes a service product role and is linked to a service specification (nfdicore:ServiceSpecification) through nfdicore:hasSpecification. This follows Pattern 1 for describing service processes, ensuring a structured linkage between processes, participants, roles, outputs, and specifications.","title":"Pattern 2: Service"},{"location":"patterns/#pattern-3-resource-contact-point","text":"The pattern represents identifying the contact point of an NFDI resource. In NFDIcore, resources are continuants that encompass various digital creative works, such as datasets, collections, metadata, and offered products/services like data portals, data curation, and data digitization. Here, an NFDI resource is linked to a contact point agent through nfdicore:hasContactPoint. The agent, participating in a process, realizes a contact point role, which is further denoted by specific contact details, like a name, email address, and website. This structured approach ensures that each NFDI resource can be linked to a responsible contact agent with clearly defined roles and accessible communication channels, enhancing transparency and usability in NFDI-related services and digital infrastructures.","title":"Pattern 3: Resource- Contact point"},{"location":"patterns/#pattern-4-resource-implementation","text":"This diagram models the implementation relationship between an resource and its associated agent. The MatWerk project is represented as a process that involves a participant (like MatWerk TA, IUC, or PP) realizing a role. The output of this project is an NFDI resource (e.g., dataset, tool, software, or digital service). The property mwo:implementedBy provides a shortcut for linking the NFDI resource to the process that produced it, simplifying the tracking of tools, services, and digital outputs within the NFDI MatWerk consortium. This property was specifically introduced to address competency questions such as which tools were developed, co-developed, or adopted in an organization, enabling a more structured representation of research outputs and their contributors.","title":"Pattern 4: Resource implementation"},{"location":"patterns/#pattern-5-resource-description","text":"The pattern presents a detailed model for describing an NFDI resource (such as software, an ontology, a document, a publication, or a service) by incorporating multiple Information Content Entities (ICEs). Each NfdiResource is linked to various attributes through object properties, which are primarily refinements of iao:is_about. These attributes can include a wide range of ICEs like title, abbreviation, subject area, file format, specification, version, license, citation, standard, repository, external identifiers, and various textual entities (e.g., author information, keywords, requirements, and affiliations). Furthermore, NFDIcore defines specific sub-properties of iao:is_about, such as hasDataset, hasFileFormat, and hasLicence, to create a structured and semantically rich representation of digital resources.","title":"Pattern 5: Resource description"},{"location":"patterns/#pattern-6-value-specification","text":"This pattern illustrates how data values associated with various ICEs are represented using the nfdicore:hasValue data property. In general, all data values related to the above-mentioned patterns can be linked to ICEs via nfdicore:hasValue, ensuring a structured approach to representing numerical or textual values. More specifically, for measurement data, the measured value is assigned using nfdicore:hasValue, while its corresponding measurement unit is linked to a unit instance of iao:MeasurementUnitLabel via mwo:hasMeasurementUnitLabel.","title":"Pattern 6: Value specification"},{"location":"patterns/#pattern-7-temporal-region","text":"This pattern illustrates the representation of temporal regions within the BFO framework, distinguishing between zero-dimensional and one-dimensional temporal regions. A bfo:ZeroDimensionalTemporalRegion represents an instant in time, such as the start or end of an event, while a bfo:OneDimensionalTemporalRegion represents a time interval that has a duration and within which events can occur. The relationships between temporal entities, such as occupies_temporal_region, has_first_instance, and has_last_instance, are derived from BFO. To represent data associated with these temporal regions, mwo:SpecificationDatum, a subclass of iao:DataItem, is used. These data points, such as start date, end date, duration, and frequency, are linked to their values using the nfdicore:hasValue data property.","title":"Pattern 7: Temporal region"},{"location":"patterns/#pattern-8-material-entity-description","text":"The diagram provides a structured representation for Material Entities (such as devices or materials) descriptions. A Material Entity is connected to its relating processes or projects via bfo:participates_in or nfdicore:isOutputOf, indicating its involvement or production within a project. The realized aspects of a Material Entity, such as its role, disposition, and function, are represented through bfo:bearer_of and bfo:material_basis_of objerct properties. Additionally, agents (organizations, persons, service providers, or contact points) linked to the material entity are included via nfdicore:hasAgent. The location of the material entity is captured using nfdicore:Place, while its structural composition is detailed with obo:has_continuant_part or obo:has_member_part. Furthermore, various ICEs are used to describe the material entity\u2019s metadata, including identifiers, specifications, and other descriptive attributes, with nfdicore:hasSpecification linking the entity to its specifications.","title":"Pattern 8: Material entity description"},{"location":"patterns/#use-cases","text":"To illustrate the practical utility of ODPs, we presented several use cases within the MSE domain, such as representing metadata for materials, instruments, large-scale facilities, researchers, organizations, educational events, resources, datasets, data portals, and material research software. Through these cases, we emphasize how ODPs facilitate development by offering standardized templates for tasks such as hierarchy modeling, semantic relationship management, and data source integration, while also supporting ontology-driven solutions that are technically sound and adaptable to changing domain requirements.","title":"Use cases"},{"location":"patterns/#use-case-1-nfdi-matwerk-organization","text":"","title":"Use case 1- NFDI MatWerk organization"},{"location":"patterns/#use-case-2-nfdi-matwerk-persons","text":"","title":"Use case 2- NFDI MatWerk persons"},{"location":"patterns/#use-case-3-nfdi-matwerk-task-area-ta","text":"","title":"Use case 3- NFDI MatWerk task area (TA)"},{"location":"patterns/#use-case-4-software-description","text":"","title":"Use case 4- Software description"},{"location":"patterns/#use-case-5-event-description","text":"","title":"Use case 5- Event description"},{"location":"patterns/#use-case-5-large-scale-facility-description","text":"","title":"Use case 5- large scale facility description"},{"location":"refs/","text":"References and publications References [1] https://www.nfdi.de/ [2] https://www.dfg.de/en/research-funding/funding-initiative/nfdi [3] https://nfdi-matwerk.de/ [4] https://nfdi-matwerk.de/project/structure/task-areas/ta-oms [5] ISO/IEC 21838-2:2021, Information technology \u2014 Top-level ontologies (TLO)Part 2: Basic Formal Ontology (BFO), https://www.iso.org/standard/74572.html . [6] Ontology Development Kit (ODK): https://incatools.github.io/ontology-development-kit/ [7] MSE Knowledge graph: https://msekg-nfdi-matwerk-02.web.vulcanus.otc.coscine.dev/matwerk/ [8] Basic Formal Ontology (BFO 2020): https://github.com/BFO-ontology/BFO-2020 [9] NFDIcore ontology (v3): https://github.com/ISE-FIZKarlsruhe/nfdicore/tree/main [10] Platform MaterialDigital Core Ontology (PMDco v3): https://github.com/materialdigital/core-ontology [11] Artifact Ontology (IAO): https://github.com/information-artifact-ontology/IAO [12] Software Ontology (SWO): https://github.com/allysonlister/swo [13] Data Catalog Vocabulary (DCAT): https://www.w3.org/TR/vocab-dcat-3/ [14] schema.org vocabulary: https://schema.org/ List of publications MWO v3.0 has/will be presented in several academic events and journal papers. The publication list will be updated soon!","title":"References and publications"},{"location":"refs/#references-and-publications","text":"","title":"References and publications"},{"location":"refs/#references","text":"[1] https://www.nfdi.de/ [2] https://www.dfg.de/en/research-funding/funding-initiative/nfdi [3] https://nfdi-matwerk.de/ [4] https://nfdi-matwerk.de/project/structure/task-areas/ta-oms [5] ISO/IEC 21838-2:2021, Information technology \u2014 Top-level ontologies (TLO)Part 2: Basic Formal Ontology (BFO), https://www.iso.org/standard/74572.html . [6] Ontology Development Kit (ODK): https://incatools.github.io/ontology-development-kit/ [7] MSE Knowledge graph: https://msekg-nfdi-matwerk-02.web.vulcanus.otc.coscine.dev/matwerk/ [8] Basic Formal Ontology (BFO 2020): https://github.com/BFO-ontology/BFO-2020 [9] NFDIcore ontology (v3): https://github.com/ISE-FIZKarlsruhe/nfdicore/tree/main [10] Platform MaterialDigital Core Ontology (PMDco v3): https://github.com/materialdigital/core-ontology [11] Artifact Ontology (IAO): https://github.com/information-artifact-ontology/IAO [12] Software Ontology (SWO): https://github.com/allysonlister/swo [13] Data Catalog Vocabulary (DCAT): https://www.w3.org/TR/vocab-dcat-3/ [14] schema.org vocabulary: https://schema.org/","title":"References"},{"location":"refs/#list-of-publications","text":"MWO v3.0 has/will be presented in several academic events and journal papers. The publication list will be updated soon!","title":"List of publications"},{"location":"versions/","text":"Versions Stable release versions The latest version of the ontology can always be found at: mwo.owl and mwo.ttl Variants The ontology is shipped in three varaints, each as OWL ( .owl) and Turtle serializations ( .ttl): full: mwo-full.ttl , mwo.ttl (default). base: mwo-base.ttl simple: mwo-simple.ttl . The \"full release\" artefact contains all logical axioms, including inferred subsumptions. All imports and components are merged into the full release artefact to ensure easy version management. The full release represents most closely the actual ontology as it was intended at the time of release, including all its logical implications. The \"base file\" is a specific release flavour. It reflects the intention of the ontology author for the official (publicly released) representation of the ontologies \"base entities\". \"Base entities\" are entities that are defined (\"owned\") by the ontology. The representation includes the intended public metadata (annotations), and classification (subClassOf hierarchy), including any statements where a base entity is the subject. The \"simple\" artefact only contains a simple existential graph of the terms defined in the ontology. This corresponds to the state before logical definitions and imports. For example, the only logical axioms are of the form CL1 subClassOf CL2 or CL1 subClassOf R some CL3 where R is any objectProperty and CLn is a class. The simple variant only contains the essential classes and no imports. The ontology \"main\" file mwo.ttl contains the full version. Editors' version Editors of this ontology should use the edit version. From this version all release variants are derived by the build workflows. Editors version: src/ontology/mwo-edit.owl","title":"Versions"},{"location":"versions/#versions","text":"","title":"Versions"},{"location":"versions/#stable-release-versions","text":"The latest version of the ontology can always be found at: mwo.owl and mwo.ttl","title":"Stable release versions"},{"location":"versions/#variants","text":"The ontology is shipped in three varaints, each as OWL ( .owl) and Turtle serializations ( .ttl): full: mwo-full.ttl , mwo.ttl (default). base: mwo-base.ttl simple: mwo-simple.ttl . The \"full release\" artefact contains all logical axioms, including inferred subsumptions. All imports and components are merged into the full release artefact to ensure easy version management. The full release represents most closely the actual ontology as it was intended at the time of release, including all its logical implications. The \"base file\" is a specific release flavour. It reflects the intention of the ontology author for the official (publicly released) representation of the ontologies \"base entities\". \"Base entities\" are entities that are defined (\"owned\") by the ontology. The representation includes the intended public metadata (annotations), and classification (subClassOf hierarchy), including any statements where a base entity is the subject. The \"simple\" artefact only contains a simple existential graph of the terms defined in the ontology. This corresponds to the state before logical definitions and imports. For example, the only logical axioms are of the form CL1 subClassOf CL2 or CL1 subClassOf R some CL3 where R is any objectProperty and CLn is a class. The simple variant only contains the essential classes and no imports. The ontology \"main\" file mwo.ttl contains the full version.","title":"Variants"},{"location":"versions/#editors-version","text":"Editors of this ontology should use the edit version. From this version all release variants are derived by the build workflows. Editors version: src/ontology/mwo-edit.owl","title":"Editors' version"},{"location":"odk-workflows/","text":"Default ODK Workflows Daily Editors Workflow Release Workflow Manage your ODK Repository Setting up Docker for ODK Imports management Managing the documentation Managing your Automated Testing","title":"Default ODK Workflows"},{"location":"odk-workflows/#default-odk-workflows","text":"Daily Editors Workflow Release Workflow Manage your ODK Repository Setting up Docker for ODK Imports management Managing the documentation Managing your Automated Testing","title":"Default ODK Workflows"},{"location":"odk-workflows/ContinuousIntegration/","text":"Introduction to Continuous Integration Workflows with ODK Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding this to your configuration file (src/ontology/mwo-odk.yaml): ci: - github_actions When updateing your repo , you will notice a new file being added: .github/workflows/qc.yml . This file contains your CI logic, so if you need to change, or add anything, this is the place! Alternatively, if your repo is in GitLab instead of GitHub, you can set up your repo with GitLab CI by adding this to your configuration file (src/ontology/mwo-odk.yaml): ci: - gitlab-ci This will add a file called .gitlab-ci.yml in the root of your repo.","title":"Introduction to Continuous Integration Workflows with ODK"},{"location":"odk-workflows/ContinuousIntegration/#introduction-to-continuous-integration-workflows-with-odk","text":"Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding this to your configuration file (src/ontology/mwo-odk.yaml): ci: - github_actions When updateing your repo , you will notice a new file being added: .github/workflows/qc.yml . This file contains your CI logic, so if you need to change, or add anything, this is the place! Alternatively, if your repo is in GitLab instead of GitHub, you can set up your repo with GitLab CI by adding this to your configuration file (src/ontology/mwo-odk.yaml): ci: - gitlab-ci This will add a file called .gitlab-ci.yml in the root of your repo.","title":"Introduction to Continuous Integration Workflows with ODK"},{"location":"odk-workflows/EditorsWorkflow/","text":"Editors Workflow The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows: Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns. Completely automated data pipeline (GitHub Actions) DROID workflow This document only covers the first editing workflow, but more will be added in the future Local editing workflow Workflow requirements: git github docker editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc 1. Create issue Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change. 2. Update main branch In your local environment (e.g. your laptop), make sure you are on the main (prev. master ) branch and ensure that you have all the upstream changes, for example: git checkout main git pull 3. Create feature branch Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases) On your command line, this looks like this: git checkout -b issue23removeprocess 4. Perform edit Using your editor of choice, perform the intended edit. For example: Prot\u00e9g\u00e9 Open src/ontology/mwo-edit.owl in Prot\u00e9g\u00e9 Make the change Save the file TextEdit Open src/ontology/mwo-edit.owl in TextEdit (or Sublime, Atom, Vim, Nano) Make the change Save the file Consider the following when making the edit. According to our development philosophy, the only places that should be manually edited are: src/ontology/mwo-edit.owl Any ROBOT templates you chose to use (the TSV files only) Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns) components (anything in src/ontology/components ), see here . Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere . Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully. 4. Check the Git diff This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking. In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line: git status git diff 5. Quality control Now it's time to run your quality control checks. This can either happen locally ( 5a ) or through your continuous integration system ( 7/5b ). 5a. Local testing If you chose to run your test locally: sh run.sh make IMP=false test This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add PAT=false to skip the potentially lengthy process of rebuilding the patterns. sh run.sh make IMP=false PAT=false test 6. Pull request When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example: git add NAMEOFCHANGEDFILES git commit -m \"Added biological process term #12\" git push -u origin issue23removeprocess Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: see #23 to link to a related ticket, or fixes #23 if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by GitHub when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change). 7/5b. Continuous Integration Testing If you didn't run and local quality control checks (see 5a ), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions More on how to set this up here . Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red. 8. Community review Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc.). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this. This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out! 9. Merge and cleanup When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request. 10. Changelog (Optional) It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc.).","title":"Editors Workflow"},{"location":"odk-workflows/EditorsWorkflow/#editors-workflow","text":"The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows: Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns. Completely automated data pipeline (GitHub Actions) DROID workflow This document only covers the first editing workflow, but more will be added in the future","title":"Editors Workflow"},{"location":"odk-workflows/EditorsWorkflow/#local-editing-workflow","text":"Workflow requirements: git github docker editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc","title":"Local editing workflow"},{"location":"odk-workflows/EditorsWorkflow/#1-create-issue","text":"Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change.","title":"1. Create issue"},{"location":"odk-workflows/EditorsWorkflow/#2-update-main-branch","text":"In your local environment (e.g. your laptop), make sure you are on the main (prev. master ) branch and ensure that you have all the upstream changes, for example: git checkout main git pull","title":"2. Update main branch"},{"location":"odk-workflows/EditorsWorkflow/#3-create-feature-branch","text":"Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases) On your command line, this looks like this: git checkout -b issue23removeprocess","title":"3. Create feature branch"},{"location":"odk-workflows/EditorsWorkflow/#4-perform-edit","text":"Using your editor of choice, perform the intended edit. For example: Prot\u00e9g\u00e9 Open src/ontology/mwo-edit.owl in Prot\u00e9g\u00e9 Make the change Save the file TextEdit Open src/ontology/mwo-edit.owl in TextEdit (or Sublime, Atom, Vim, Nano) Make the change Save the file Consider the following when making the edit. According to our development philosophy, the only places that should be manually edited are: src/ontology/mwo-edit.owl Any ROBOT templates you chose to use (the TSV files only) Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns) components (anything in src/ontology/components ), see here . Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere . Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully.","title":"4. Perform edit"},{"location":"odk-workflows/EditorsWorkflow/#4-check-the-git-diff","text":"This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking. In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line: git status git diff","title":"4. Check the Git diff"},{"location":"odk-workflows/EditorsWorkflow/#5-quality-control","text":"Now it's time to run your quality control checks. This can either happen locally ( 5a ) or through your continuous integration system ( 7/5b ).","title":"5. Quality control"},{"location":"odk-workflows/EditorsWorkflow/#5a-local-testing","text":"If you chose to run your test locally: sh run.sh make IMP=false test This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add PAT=false to skip the potentially lengthy process of rebuilding the patterns. sh run.sh make IMP=false PAT=false test","title":"5a. Local testing"},{"location":"odk-workflows/EditorsWorkflow/#6-pull-request","text":"When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example: git add NAMEOFCHANGEDFILES git commit -m \"Added biological process term #12\" git push -u origin issue23removeprocess Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: see #23 to link to a related ticket, or fixes #23 if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by GitHub when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change).","title":"6. Pull request"},{"location":"odk-workflows/EditorsWorkflow/#75b-continuous-integration-testing","text":"If you didn't run and local quality control checks (see 5a ), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions More on how to set this up here . Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red.","title":"7/5b. Continuous Integration Testing"},{"location":"odk-workflows/EditorsWorkflow/#8-community-review","text":"Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc.). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this. This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out!","title":"8. Community review"},{"location":"odk-workflows/EditorsWorkflow/#9-merge-and-cleanup","text":"When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request.","title":"9. Merge and cleanup"},{"location":"odk-workflows/EditorsWorkflow/#10-changelog-optional","text":"It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc.).","title":"10. Changelog (Optional)"},{"location":"odk-workflows/ManageDocumentation/","text":"Updating the Documentation The documentation for MWO is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) the mkdocs.yaml file contains the documentation config, in particular its navigation bar and theme. The documentation is hosted using GitHub pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch. Editing the docs Changing content All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow: Open the .md file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker . Perform the edit and save the file Commit the file to a branch, and create a pull request as usual. If your development team likes your changes, merge the docs into main branch. Deploy the documentation (see below) Deploy the documentation The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps: In your terminal, navigate to the edit directory of your ontology, e.g.: cd mwo/src/ontology Now you are ready to build the docs as follows: sh run.sh make update_docs Mkdocs now sets off to build the site from the markdown pages. You will be asked to Enter your username Enter your password (see here for using GitHub access tokens instead) IMPORTANT : Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens! If everything was successful, you will see a message similar to this one: INFO - Your documentation should shortly be available at: https://ISE-FIZKarlsruhe.github.io/MatWerk_ontology/ 3. Just to double check, you can now navigate to your documentation pages (usually https://ISE-FIZKarlsruhe.github.io/MatWerk_ontology/). Just make sure you give GitHub 2-5 minutes to build the pages!","title":"Updating the Documentation"},{"location":"odk-workflows/ManageDocumentation/#updating-the-documentation","text":"The documentation for MWO is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) the mkdocs.yaml file contains the documentation config, in particular its navigation bar and theme. The documentation is hosted using GitHub pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch.","title":"Updating the Documentation"},{"location":"odk-workflows/ManageDocumentation/#editing-the-docs","text":"","title":"Editing the docs"},{"location":"odk-workflows/ManageDocumentation/#changing-content","text":"All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow: Open the .md file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker . Perform the edit and save the file Commit the file to a branch, and create a pull request as usual. If your development team likes your changes, merge the docs into main branch. Deploy the documentation (see below)","title":"Changing content"},{"location":"odk-workflows/ManageDocumentation/#deploy-the-documentation","text":"The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps: In your terminal, navigate to the edit directory of your ontology, e.g.: cd mwo/src/ontology Now you are ready to build the docs as follows: sh run.sh make update_docs Mkdocs now sets off to build the site from the markdown pages. You will be asked to Enter your username Enter your password (see here for using GitHub access tokens instead) IMPORTANT : Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens! If everything was successful, you will see a message similar to this one: INFO - Your documentation should shortly be available at: https://ISE-FIZKarlsruhe.github.io/MatWerk_ontology/ 3. Just to double check, you can now navigate to your documentation pages (usually https://ISE-FIZKarlsruhe.github.io/MatWerk_ontology/). Just make sure you give GitHub 2-5 minutes to build the pages!","title":"Deploy the documentation"},{"location":"odk-workflows/ReleaseWorkflow/","text":"The release workflow The release workflow recommended by the ODK is based on GitHub releases and works as follows: Run a release with the ODK Review the release Merge to main branch Create a GitHub release These steps are outlined in detail in the following. Run a release with the ODK Preparation: Ensure that all your pull requests are merged into your main (master) branch Make sure that all changes to main are committed to GitHub ( git status should say that there are no modified files) Locally make sure you have the latest changes from main ( git pull ) Checkout a new branch (e.g. git checkout -b release-2021-01-01 ) You may or may not want to refresh your imports as part of your release strategy (see here ) Make sure you have the latest ODK installed by running docker pull obolibrary/odkfull To actually run the release, you: Open a command line terminal window and navigate to the src/ontology directory ( cd mwo/src/ontology ) Run release pipeline: sh run.sh make prepare_release -B . Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI. If everything went well, you should see the following output on your machine: Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab . This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo). Review the release (Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (mwo.owl) in you favourite development environment (i.e. Prot\u00e9g\u00e9) and eyeball the hierarchy. We recommend two simple checks: Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly. Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. Commit your changes to the branch and make a pull request In your GitHub pull request, review the following three files in detail (based on our experience): mwo.obo - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review! mwo-base.owl - this reflects the asserted axioms in your ontology that you have actually edited. Ideally also take a look at mwo-full.owl , which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large. Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR! Merge the main branch Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished). Create a GitHub release Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/ISE-FIZKarlsruhe/MatWerk_ontology/releases). Then click \"Draft new release\" As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the mwo.obo file and check the data-version: property. The date needs to be prefixed with a v , so, for example v2020-02-06 . You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions. Click \"Publish release\". Done. Debugging typical ontology release problems Problems with memory When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here . Problems when using OBO format based tools Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations ( def , comment ) which are illegal in OBO. Here is an example recipe of how to deal with such a problem: If you get a message like make: *** [cl.Makefile:84: oort] Error 255 you might have a OORT error. To debug this, in your terminal enter sh run.sh make IMP=false PAT=false oort -B (assuming you are already in the ontology folder in your directory) This should show you where the error is in the log (eg multiple different definitions) WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE Open mwo-edit.owl in Prot\u00e9g\u00e9 and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save. *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in. Rerun sh run.sh make IMP=false PAT=false oort -B and if it all passes, commit your changes to a branch and make a pull request as usual.","title":"The release workflow"},{"location":"odk-workflows/ReleaseWorkflow/#the-release-workflow","text":"The release workflow recommended by the ODK is based on GitHub releases and works as follows: Run a release with the ODK Review the release Merge to main branch Create a GitHub release These steps are outlined in detail in the following.","title":"The release workflow"},{"location":"odk-workflows/ReleaseWorkflow/#run-a-release-with-the-odk","text":"Preparation: Ensure that all your pull requests are merged into your main (master) branch Make sure that all changes to main are committed to GitHub ( git status should say that there are no modified files) Locally make sure you have the latest changes from main ( git pull ) Checkout a new branch (e.g. git checkout -b release-2021-01-01 ) You may or may not want to refresh your imports as part of your release strategy (see here ) Make sure you have the latest ODK installed by running docker pull obolibrary/odkfull To actually run the release, you: Open a command line terminal window and navigate to the src/ontology directory ( cd mwo/src/ontology ) Run release pipeline: sh run.sh make prepare_release -B . Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI. If everything went well, you should see the following output on your machine: Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab . This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo).","title":"Run a release with the ODK"},{"location":"odk-workflows/ReleaseWorkflow/#review-the-release","text":"(Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (mwo.owl) in you favourite development environment (i.e. Prot\u00e9g\u00e9) and eyeball the hierarchy. We recommend two simple checks: Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly. Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. Commit your changes to the branch and make a pull request In your GitHub pull request, review the following three files in detail (based on our experience): mwo.obo - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review! mwo-base.owl - this reflects the asserted axioms in your ontology that you have actually edited. Ideally also take a look at mwo-full.owl , which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large. Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR!","title":"Review the release"},{"location":"odk-workflows/ReleaseWorkflow/#merge-the-main-branch","text":"Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished).","title":"Merge the main branch"},{"location":"odk-workflows/ReleaseWorkflow/#create-a-github-release","text":"Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/ISE-FIZKarlsruhe/MatWerk_ontology/releases). Then click \"Draft new release\" As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the mwo.obo file and check the data-version: property. The date needs to be prefixed with a v , so, for example v2020-02-06 . You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions. Click \"Publish release\". Done.","title":"Create a GitHub release"},{"location":"odk-workflows/ReleaseWorkflow/#debugging-typical-ontology-release-problems","text":"","title":"Debugging typical ontology release problems"},{"location":"odk-workflows/ReleaseWorkflow/#problems-with-memory","text":"When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here .","title":"Problems with memory"},{"location":"odk-workflows/ReleaseWorkflow/#problems-when-using-obo-format-based-tools","text":"Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations ( def , comment ) which are illegal in OBO. Here is an example recipe of how to deal with such a problem: If you get a message like make: *** [cl.Makefile:84: oort] Error 255 you might have a OORT error. To debug this, in your terminal enter sh run.sh make IMP=false PAT=false oort -B (assuming you are already in the ontology folder in your directory) This should show you where the error is in the log (eg multiple different definitions) WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE Open mwo-edit.owl in Prot\u00e9g\u00e9 and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save. *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in. Rerun sh run.sh make IMP=false PAT=false oort -B and if it all passes, commit your changes to a branch and make a pull request as usual.","title":"Problems when using OBO format based tools"},{"location":"odk-workflows/RepoManagement/","text":"Managing your ODK repository Updating your ODK repository Your ODK repositories configuration is managed in src/ontology/mwo-odk.yaml . The ODK Project Configuration Schema defines all possible parameters that can be used in this config YAML. Once you have made your changes, you can run the following to apply your changes to the repository: sh run.sh make update_repo There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here. NOTE for Windows users: You may get a cryptic failure such as Set Illegal Option - if the update script located in src/scripts/update_repo.sh was saved using Windows Line endings. These need to change to unix line endings. In Notepad++, for example, you can click on Edit->EOL Conversion->Unix LF to change this. Managing imports You can use the update repository workflow described on this page to perform the following operations to your imports: Add a new import Modify an existing import Remove an import you no longer want Customise an import We will discuss all these workflows in the following. Add new import To add a new import, you first edit your odk config as described above , adding an id to the product list in the import_group section (for the sake of this example, we assume you already import RO, and your goal is to also import GO): import_group: products: - id: ro - id: go Note: our ODK file should only have one import_group which can contain multiple imports (in the products section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here . To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps: Add an import statement to your src/ontology/mwo-edit.owl file. We suggest to do this using a text editor, by simply copying an existing import declaration and renaming it to the new ontology import, for example as follows: ... Ontology(<http://purls.helmholtz-metadaten.de/mwo/mwo.owl> Import(<http://purls.helmholtz-metadaten.de/mwo/mwo/imports/ro_import.owl>) Import(<http://purls.helmholtz-metadaten.de/mwo/mwo/imports/go_import.owl>) ... Add your imports redirect to your catalog file src/ontology/catalog-v001.xml , for example: <uri name=\"http://purl.obolibrary.org/obo/mwo/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> Test whether everything is in order: Refresh your import Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported. Note: The catalog file src/ontology/catalog-v001.xml has one purpose: redirecting imports from URLs to local files. For example, if you have Import(<http://purl.obolibrary.org/obo/mwo/imports/go_import.owl>) in your editors file (the ontology) and <uri name=\"http://purls.helmholtz-metadaten.de/mwo/mwo/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> in your catalog, tools like robot or Prot\u00e9g\u00e9 will recognize the statement in the catalog file to redirect the URL http://purl.obolibrary.org/obo/mwo/imports/go_import.owl to the local file imports/go_import.owl (which is in your src/ontology directory). Modify an existing import If you simply wish to refresh your import in light of new terms, see here . If you wish to change the type of your module see section \"Customise an import\". Remove an existing import To remove an existing import, perform the following steps: remove the import declaration from your src/ontology/mwo-edit.owl . remove the id from your src/ontology/mwo-odk.yaml , eg. - id: go from the list of products in the import_group . run update repo workflow delete the associated files manually: src/imports/go_import.owl src/imports/go_terms.txt Remove the respective entry from the src/ontology/catalog-v001.xml file. Customise an import By default, an import module extracted from a source ontology will be a SLME module, see here . There are various options to change the default. The following change to your repo config ( src/ontology/mwo-odk.yaml ) will switch the go import from an SLME module to a simple ROBOT filter module: import_group: products: - id: ro - id: go module_type: filter A ROBOT filter module is, essentially, importing all external terms declared by your ontology (see here on how to declare external terms to be imported). Note that the filter module does not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This behaviour can be changed by adding additional base IRIs as follows: import_group: products: - id: go module_type: filter base_iris: - http://purl.obolibrary.org/obo/GO_ - http://purl.obolibrary.org/obo/CL_ - http://purl.obolibrary.org/obo/BFO If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config ( src/ontology/mwo-odk.yaml ): import_group: products: - id: ro - id: go module_type: custom Now add a new goal in your custom Makefile ( src/ontology/mwo.Makefile , not src/ontology/Makefile ). imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only: extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ to another ROBOT pipeline. Add a component A component is an import which belongs to your ontology, e.g. is managed by you and your team. Open src/ontology/mwo-odk.yaml If you dont have it yet, add a new top level section components Under the components section, add a new section called products . This is where all your components are specified Under the products section, add a new component, e.g. - filename: mycomp.owl Example components: products: - filename: mycomp.owl When running sh run.sh make update_repo , a new file src/ontology/components/mycomp.owl will be created which you can edit as you see fit. Typical ways to edit: Using a ROBOT template to generate the component (see below) Manually curating the component separately with Prot\u00e9g\u00e9 or any other editor Providing a components/mycomp.owl: make target in src/ontology/mwo.Makefile and provide a custom command to generate the component WARNING : Note that the custom rule to generate the component MUST NOT depend on any other ODK-generated file such as seed files and the like (see issue ). Providing an additional attribute for the component in src/ontology/mwo-odk.yaml , source , to specify that this component should simply be downloaded from somewhere on the web. Adding a new component based on a ROBOT template Since ODK 1.3.2, it is possible to simply link a ROBOT template to a component without having to specify any of the import logic. In order to add a new component that is connected to one or more template files, follow these steps: Open src/ontology/mwo-odk.yaml . Make sure that use_templates: TRUE is set in the global project options. You should also make sure that use_context: TRUE is set in case you are using prefixes in your templates that are not known to robot , such as OMOP: , CPONT: and more. All non-standard prefixes you are using should be added to config/context.json . Add another component to the products section. To activate this component to be template-driven, simply say: use_template: TRUE . This will create an empty template for you in the templates directory, which will automatically be processed when recreating the component (e.g. run.bat make recreate-mycomp ). If you want to use more than one component, use the templates field to add as many template names as you wish. ODK will look for them in the src/templates directory. Advanced: If you want to provide additional processing options, you can use the template_options field. This should be a string with option from robot template . One typical example for additional options you may want to provide is --add-prefixes config/context.json to ensure the prefix map of your context is provided to robot , see above. Example : components: products: - filename: mycomp.owl use_template: TRUE template_options: --add-prefixes config/context.json templates: - template1.tsv - template2.tsv Note : if your mirror is particularly large and complex, read this ODK recommendation .","title":"Managing your ODK repository"},{"location":"odk-workflows/RepoManagement/#managing-your-odk-repository","text":"","title":"Managing your ODK repository"},{"location":"odk-workflows/RepoManagement/#updating-your-odk-repository","text":"Your ODK repositories configuration is managed in src/ontology/mwo-odk.yaml . The ODK Project Configuration Schema defines all possible parameters that can be used in this config YAML. Once you have made your changes, you can run the following to apply your changes to the repository: sh run.sh make update_repo There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here. NOTE for Windows users: You may get a cryptic failure such as Set Illegal Option - if the update script located in src/scripts/update_repo.sh was saved using Windows Line endings. These need to change to unix line endings. In Notepad++, for example, you can click on Edit->EOL Conversion->Unix LF to change this.","title":"Updating your ODK repository"},{"location":"odk-workflows/RepoManagement/#managing-imports","text":"You can use the update repository workflow described on this page to perform the following operations to your imports: Add a new import Modify an existing import Remove an import you no longer want Customise an import We will discuss all these workflows in the following.","title":"Managing imports"},{"location":"odk-workflows/RepoManagement/#add-new-import","text":"To add a new import, you first edit your odk config as described above , adding an id to the product list in the import_group section (for the sake of this example, we assume you already import RO, and your goal is to also import GO): import_group: products: - id: ro - id: go Note: our ODK file should only have one import_group which can contain multiple imports (in the products section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here . To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps: Add an import statement to your src/ontology/mwo-edit.owl file. We suggest to do this using a text editor, by simply copying an existing import declaration and renaming it to the new ontology import, for example as follows: ... Ontology(<http://purls.helmholtz-metadaten.de/mwo/mwo.owl> Import(<http://purls.helmholtz-metadaten.de/mwo/mwo/imports/ro_import.owl>) Import(<http://purls.helmholtz-metadaten.de/mwo/mwo/imports/go_import.owl>) ... Add your imports redirect to your catalog file src/ontology/catalog-v001.xml , for example: <uri name=\"http://purl.obolibrary.org/obo/mwo/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> Test whether everything is in order: Refresh your import Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported. Note: The catalog file src/ontology/catalog-v001.xml has one purpose: redirecting imports from URLs to local files. For example, if you have Import(<http://purl.obolibrary.org/obo/mwo/imports/go_import.owl>) in your editors file (the ontology) and <uri name=\"http://purls.helmholtz-metadaten.de/mwo/mwo/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> in your catalog, tools like robot or Prot\u00e9g\u00e9 will recognize the statement in the catalog file to redirect the URL http://purl.obolibrary.org/obo/mwo/imports/go_import.owl to the local file imports/go_import.owl (which is in your src/ontology directory).","title":"Add new import"},{"location":"odk-workflows/RepoManagement/#modify-an-existing-import","text":"If you simply wish to refresh your import in light of new terms, see here . If you wish to change the type of your module see section \"Customise an import\".","title":"Modify an existing import"},{"location":"odk-workflows/RepoManagement/#remove-an-existing-import","text":"To remove an existing import, perform the following steps: remove the import declaration from your src/ontology/mwo-edit.owl . remove the id from your src/ontology/mwo-odk.yaml , eg. - id: go from the list of products in the import_group . run update repo workflow delete the associated files manually: src/imports/go_import.owl src/imports/go_terms.txt Remove the respective entry from the src/ontology/catalog-v001.xml file.","title":"Remove an existing import"},{"location":"odk-workflows/RepoManagement/#customise-an-import","text":"By default, an import module extracted from a source ontology will be a SLME module, see here . There are various options to change the default. The following change to your repo config ( src/ontology/mwo-odk.yaml ) will switch the go import from an SLME module to a simple ROBOT filter module: import_group: products: - id: ro - id: go module_type: filter A ROBOT filter module is, essentially, importing all external terms declared by your ontology (see here on how to declare external terms to be imported). Note that the filter module does not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This behaviour can be changed by adding additional base IRIs as follows: import_group: products: - id: go module_type: filter base_iris: - http://purl.obolibrary.org/obo/GO_ - http://purl.obolibrary.org/obo/CL_ - http://purl.obolibrary.org/obo/BFO If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config ( src/ontology/mwo-odk.yaml ): import_group: products: - id: ro - id: go module_type: custom Now add a new goal in your custom Makefile ( src/ontology/mwo.Makefile , not src/ontology/Makefile ). imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only: extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ to another ROBOT pipeline.","title":"Customise an import"},{"location":"odk-workflows/RepoManagement/#add-a-component","text":"A component is an import which belongs to your ontology, e.g. is managed by you and your team. Open src/ontology/mwo-odk.yaml If you dont have it yet, add a new top level section components Under the components section, add a new section called products . This is where all your components are specified Under the products section, add a new component, e.g. - filename: mycomp.owl Example components: products: - filename: mycomp.owl When running sh run.sh make update_repo , a new file src/ontology/components/mycomp.owl will be created which you can edit as you see fit. Typical ways to edit: Using a ROBOT template to generate the component (see below) Manually curating the component separately with Prot\u00e9g\u00e9 or any other editor Providing a components/mycomp.owl: make target in src/ontology/mwo.Makefile and provide a custom command to generate the component WARNING : Note that the custom rule to generate the component MUST NOT depend on any other ODK-generated file such as seed files and the like (see issue ). Providing an additional attribute for the component in src/ontology/mwo-odk.yaml , source , to specify that this component should simply be downloaded from somewhere on the web.","title":"Add a component"},{"location":"odk-workflows/RepoManagement/#adding-a-new-component-based-on-a-robot-template","text":"Since ODK 1.3.2, it is possible to simply link a ROBOT template to a component without having to specify any of the import logic. In order to add a new component that is connected to one or more template files, follow these steps: Open src/ontology/mwo-odk.yaml . Make sure that use_templates: TRUE is set in the global project options. You should also make sure that use_context: TRUE is set in case you are using prefixes in your templates that are not known to robot , such as OMOP: , CPONT: and more. All non-standard prefixes you are using should be added to config/context.json . Add another component to the products section. To activate this component to be template-driven, simply say: use_template: TRUE . This will create an empty template for you in the templates directory, which will automatically be processed when recreating the component (e.g. run.bat make recreate-mycomp ). If you want to use more than one component, use the templates field to add as many template names as you wish. ODK will look for them in the src/templates directory. Advanced: If you want to provide additional processing options, you can use the template_options field. This should be a string with option from robot template . One typical example for additional options you may want to provide is --add-prefixes config/context.json to ensure the prefix map of your context is provided to robot , see above. Example : components: products: - filename: mycomp.owl use_template: TRUE template_options: --add-prefixes config/context.json templates: - template1.tsv - template2.tsv Note : if your mirror is particularly large and complex, read this ODK recommendation .","title":"Adding a new component based on a ROBOT template"},{"location":"odk-workflows/RepositoryFileStructure/","text":"Repository structure The main kinds of files in the repository: Release files Imports Components Release files Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed description of the release artefacts can be found here . Imports Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain. These are the current imports in MWO Import URL Type iao http://purl.obolibrary.org/obo/iao.owl custom nfdicore https://raw.githubusercontent.com/ISE-FIZKarlsruhe/nfdicore/refs/heads/main/nfdicore.ttl mirror pmdco https://raw.githubusercontent.com/materialdigital/core-ontology/refs/heads/main/pmdco.owl custom Components Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases: There is an automated process that generates and re-generates a part of the ontology A part of the ontology is managed in ROBOT templates The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may choose to manage logic that is beyond OBO in a specific OWL component.","title":"Repository structure"},{"location":"odk-workflows/RepositoryFileStructure/#repository-structure","text":"The main kinds of files in the repository: Release files Imports Components","title":"Repository structure"},{"location":"odk-workflows/RepositoryFileStructure/#release-files","text":"Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed description of the release artefacts can be found here .","title":"Release files"},{"location":"odk-workflows/RepositoryFileStructure/#imports","text":"Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain. These are the current imports in MWO Import URL Type iao http://purl.obolibrary.org/obo/iao.owl custom nfdicore https://raw.githubusercontent.com/ISE-FIZKarlsruhe/nfdicore/refs/heads/main/nfdicore.ttl mirror pmdco https://raw.githubusercontent.com/materialdigital/core-ontology/refs/heads/main/pmdco.owl custom","title":"Imports"},{"location":"odk-workflows/RepositoryFileStructure/#components","text":"Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases: There is an automated process that generates and re-generates a part of the ontology A part of the ontology is managed in ROBOT templates The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may choose to manage logic that is beyond OBO in a specific OWL component.","title":"Components"},{"location":"odk-workflows/SettingUpDockerForODK/","text":"Setting up your Docker environment for ODK use One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/mwo-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting up your Docker environment for ODK use"},{"location":"odk-workflows/SettingUpDockerForODK/#setting-up-your-docker-environment-for-odk-use","text":"One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/mwo-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting up your Docker environment for ODK use"},{"location":"odk-workflows/UpdateImports/","text":"Update Imports Workflow This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here . Importing a new term Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\". Importing a new term is split into two sub-phases: Declaring the terms to be imported Refreshing imports dynamically Declaring terms to be imported There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Prot\u00e9g\u00e9-based declaration Using term files Using the custom import template Prot\u00e9g\u00e9-based declaration This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . This approach also applies to ontologies that use base module import approach. Open your ontology (edit file) in Prot\u00e9g\u00e9 (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc.) for this term are imported from the respective external source ontology and becomes visible in your ontology. Using term files Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported. Using the custom import template This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/mwo-odk.yaml ), and update the repository : use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say currently import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extend the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file . Refresh imports If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed . sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B Using the Base Module approach Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it . A base file is a subset of the ontology that only contains those axioms that nominally belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this: Imagine this being the full Uberon ontology: Axiom 1: BFO:123 SubClassOf BFO:124 Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 The base file is the set of all axioms that are about UBERON terms: Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 I.e. Axiom 1: BFO:123 SubClassOf BFO:124 Gets removed. The base file pipeline is a bit more complex than the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first merging all mirrors into one huge file and then extracting one mega module from it. Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we 1) First obtain the base (usually by simply downloading it, but there is also an option now to create it with ROBOT) 2) We merge all base files into one big pile 3) Then we extract a single module imports/merged_import.owl The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml. To check if your ontology uses this method, check src/ontology/mwo-odk.yaml to see if use_base_merging: TRUE is declared under import_group If your ontology uses Base Module approach, please use the following steps: First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you) Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then refresh imports by running sh run.sh make imports/merged_import.owl Note: if your mirrors are updated, you can run sh run.sh make no-mirror-refresh-merged This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and one of the local devs should pick this up and run the import for you. Lastly, restart Prot\u00e9g\u00e9, and the term should be imported in ready to be used.","title":"Update Imports Workflow"},{"location":"odk-workflows/UpdateImports/#update-imports-workflow","text":"This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here .","title":"Update Imports Workflow"},{"location":"odk-workflows/UpdateImports/#importing-a-new-term","text":"Note: some ontologies now use a merged-import system to manage dynamic imports, for these please follow instructions in the section title \"Using the Base Module approach\". Importing a new term is split into two sub-phases: Declaring the terms to be imported Refreshing imports dynamically","title":"Importing a new term"},{"location":"odk-workflows/UpdateImports/#declaring-terms-to-be-imported","text":"There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Prot\u00e9g\u00e9-based declaration Using term files Using the custom import template","title":"Declaring terms to be imported"},{"location":"odk-workflows/UpdateImports/#protege-based-declaration","text":"This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . This approach also applies to ontologies that use base module import approach. Open your ontology (edit file) in Prot\u00e9g\u00e9 (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc.) for this term are imported from the respective external source ontology and becomes visible in your ontology.","title":"Prot\u00e9g\u00e9-based declaration"},{"location":"odk-workflows/UpdateImports/#using-term-files","text":"Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported.","title":"Using term files"},{"location":"odk-workflows/UpdateImports/#using-the-custom-import-template","text":"This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/mwo-odk.yaml ), and update the repository : use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say currently import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extend the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file .","title":"Using the custom import template"},{"location":"odk-workflows/UpdateImports/#refresh-imports","text":"If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. Note: You must have docker installed . sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B","title":"Refresh imports"},{"location":"odk-workflows/UpdateImports/#using-the-base-module-approach","text":"Since ODK 1.2.31, we support an entirely new approach to generate modules: Using base files. The idea is to only import axioms from ontologies that actually belong to it . A base file is a subset of the ontology that only contains those axioms that nominally belong there. In other words, the base file does not contain any axioms that belong to another ontology. An example would be this: Imagine this being the full Uberon ontology: Axiom 1: BFO:123 SubClassOf BFO:124 Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 The base file is the set of all axioms that are about UBERON terms: Axiom 1: UBERON:123 SubClassOf BFO:123 Axiom 1: UBERON:124 SubClassOf UBERON 123 I.e. Axiom 1: BFO:123 SubClassOf BFO:124 Gets removed. The base file pipeline is a bit more complex than the normal pipelines, because of the logical interactions between the imported ontologies. This is solved by _first merging all mirrors into one huge file and then extracting one mega module from it. Example: Let's say we are importing terms from Uberon, GO and RO in our ontologies. When we use the base pipelines, we 1) First obtain the base (usually by simply downloading it, but there is also an option now to create it with ROBOT) 2) We merge all base files into one big pile 3) Then we extract a single module imports/merged_import.owl The first implementation of this pipeline is PATO, see https://github.com/pato-ontology/pato/blob/master/src/ontology/pato-odk.yaml. To check if your ontology uses this method, check src/ontology/mwo-odk.yaml to see if use_base_merging: TRUE is declared under import_group If your ontology uses Base Module approach, please use the following steps: First, add the term to be imported to the term file associated with it (see above \"Using term files\" section if this is not clear to you) Next, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then refresh imports by running sh run.sh make imports/merged_import.owl Note: if your mirrors are updated, you can run sh run.sh make no-mirror-refresh-merged This requires quite a bit of memory on your local machine, so if you encounter an error, it might be a lack of memory on your computer. A solution would be to create a ticket in an issue tracker requesting for the term to be imported, and one of the local devs should pick this up and run the import for you. Lastly, restart Prot\u00e9g\u00e9, and the term should be imported in ready to be used.","title":"Using the Base Module approach"},{"location":"odk-workflows/components/","text":"Adding components to an ODK repo For details on what components are, please see component section of repository file structure document . To add custom components to an ODK repo, please follow the following steps: 1) Locate your odk yaml file and open it with your favourite text editor (src/ontology/mwo-odk.yaml) 2) Search if there is already a component section to the yaml file, if not add it accordingly, adding the name of your component: components: products: - filename: your-component-name.owl 3) Add the component to your catalog file (src/ontology/catalog-v001.xml) <uri name=\"http://purls.helmholtz-metadaten.de/mwo/mwo/components/your-component-name.owl\" uri=\"components/your-component-name.owl\"/> 4) Add the component to the edit file (src/ontology/mwo-edit.obo) for .obo formats: import: http://purls.helmholtz-metadaten.de/mwo/mwo/components/your-component-name.owl for .owl formats: Import(<http://purls.helmholtz-metadaten.de/mwo/mwo/components/your-component-name.owl>) 5) Refresh your repo by running sh run.sh make update_repo - this should create a new file in src/ontology/components. 6) In your custom makefile (src/ontology/mwo.Makefile) add a goal for your custom make file. In this example, the goal is a ROBOT template. $(COMPONENTSDIR)/your-component-name.owl: $(SRC) ../templates/your-component-template.tsv $(ROBOT) template --template ../templates/your-component-template.tsv \\ annotate --ontology-iri $(ONTBASE)/$@ --output $(COMPONENTSDIR)/your-component-name.owl (If using a ROBOT template, do not forget to add your template tsv in src/templates/) 7) Make the file by running sh run.sh make components/your-component-name.owl","title":"Adding components to an ODK repo"},{"location":"odk-workflows/components/#adding-components-to-an-odk-repo","text":"For details on what components are, please see component section of repository file structure document . To add custom components to an ODK repo, please follow the following steps: 1) Locate your odk yaml file and open it with your favourite text editor (src/ontology/mwo-odk.yaml) 2) Search if there is already a component section to the yaml file, if not add it accordingly, adding the name of your component: components: products: - filename: your-component-name.owl 3) Add the component to your catalog file (src/ontology/catalog-v001.xml) <uri name=\"http://purls.helmholtz-metadaten.de/mwo/mwo/components/your-component-name.owl\" uri=\"components/your-component-name.owl\"/> 4) Add the component to the edit file (src/ontology/mwo-edit.obo) for .obo formats: import: http://purls.helmholtz-metadaten.de/mwo/mwo/components/your-component-name.owl for .owl formats: Import(<http://purls.helmholtz-metadaten.de/mwo/mwo/components/your-component-name.owl>) 5) Refresh your repo by running sh run.sh make update_repo - this should create a new file in src/ontology/components. 6) In your custom makefile (src/ontology/mwo.Makefile) add a goal for your custom make file. In this example, the goal is a ROBOT template. $(COMPONENTSDIR)/your-component-name.owl: $(SRC) ../templates/your-component-template.tsv $(ROBOT) template --template ../templates/your-component-template.tsv \\ annotate --ontology-iri $(ONTBASE)/$@ --output $(COMPONENTSDIR)/your-component-name.owl (If using a ROBOT template, do not forget to add your template tsv in src/templates/) 7) Make the file by running sh run.sh make components/your-component-name.owl","title":"Adding components to an ODK repo"}]}